{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eca1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sbn \n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "#load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('real_estate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c24309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f404a1",
   "metadata": {},
   "source": [
    "it shows that there are not null values or missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a14c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= data.copy()\n",
    "#Create the feature year and month\n",
    "df['year'] = df['transaction_date'].astype('int')\n",
    "df['month'] = ((df['transaction_date'] - df['year']) * 12).astype('int') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd98d7f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "df= df.drop('transaction_date',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b3fef",
   "metadata": {},
   "source": [
    "Now check the outliers for everthing but we only going to focus in the variable that we need predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6678d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotbox(e):\n",
    "    for i in ((e.columns.values)):\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sbn.boxplot(x=e[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fe8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.histplot(df['price_per_unit'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotbox(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa21915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The boxplots and histplot show that there are some outliers in transit_distance and in price_per_unit features \n",
    "#But let's focus in price_per_unit feature\n",
    "#We will remove the outliers using the IQR method\n",
    "#First we need to calculate the IQR\n",
    "q1= df['price_per_unit'].quantile(0.25)\n",
    "q3 = df['price_per_unit'].quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856108bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr= q3 - q1\n",
    "#Then set the upper and lower limit\n",
    "upper_limit= q3 + (1.5*iqr)\n",
    "lower_limit = q1 - (1.5*iqr)\n",
    "#Then remove the outliers\n",
    "df.loc[df['price_per_unit'] > upper_limit, 'price_per_unit'] = upper_limit\n",
    "df.loc[df['price_per_unit'] < 0, 'price_per_unit'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.histplot(df['price_per_unit'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f570f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotbox(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8857b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have removed the outliers and we have a normal distribution in price_per_unit feature\n",
    "#Now let's check the numeric correlations\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Correlation Graph')\n",
    "sbn.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Like year and month doesn't have a great correlation let's ignore this\n",
    "df= df.drop(['year','month'],axis=1)\n",
    "#Now that we have the possible corelation between features let's split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a45f0a1",
   "metadata": {},
   "source": [
    "We will use the train_test_split function from sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60230df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df.drop('price_per_unit',axis=1)\n",
    "y= df['price_per_unit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2143eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.30 , random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5d121",
   "metadata": {},
   "source": [
    "Like we want to predict a numeric features but in the correlation map it shows that the features don't have to significant correlation then let's try a Random Forest Regressor\n",
    "We will use the RandomForestRegressor from sklearn library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80db6b44",
   "metadata": {},
   "source": [
    "But first let's preproccess the data like scale the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal way\n",
    "'''\n",
    "scaler= StandardScaler()\n",
    "\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "\n",
    "x_test_scaled = scaler.transform(x_train)\n",
    "\n",
    "randomForest=  RandomForestRegressor(random_state=42)\n",
    "\n",
    "randomForest.fit(x_train_scaled,y_train)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2b18f",
   "metadata": {},
   "source": [
    "Using pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124bb3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing for numeric columns (scale them)\n",
    "numeric_features = [0,1,2,3,4]\n",
    "numeric_transformer= Pipeline(steps=[('scaler',StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a360b8fd",
   "metadata": {},
   "source": [
    "Combining preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411169aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltype= RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing and training pipeline\n",
    "pipeline=  Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', modeltype)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71fcbc6",
   "metadata": {},
   "source": [
    "fit the pipeline to train a linear regression model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1deb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRandomForest= pipeline.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd522a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "predictRF=  modelRandomForest.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3d7fcc",
   "metadata": {},
   "source": [
    "To evaluate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc7c59",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def scoresRegressor(pred,d_test):\n",
    "         mse = mean_squared_error(d_test, pred)\n",
    "         print(\"MSE: {0}\".format(round(mse,2)))\n",
    "         rmse = np.sqrt(mse)\n",
    "         print(\"RMSE: {0}\".format(round(rmse,2)))\n",
    "         r2 = r2_score(y_test, pred)\n",
    "         print(\"R2: {0}\".format(round(r2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6dece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresRegressor(predictRF,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea0d67",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "And now is time to plot the predict values vs the true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a4743",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sbn.regplot(x=y_test,y= predictRF,line_kws=dict(color=\"r\"))\n",
    "plt.title('Predicted  vs True Values')\n",
    "plt.xlabel('predicted values')\n",
    "plt.ylabel('actual values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fbe39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as a pickle file\n",
    "filename = './real_estate_model.pkl'\n",
    "joblib.dump(modelRandomForest, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "loaded_model = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[16.2,289.3248,5,24.98203,121.54348],\n",
    "                  [13.6,4082.015,0,24.94155,121.5038]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9f94e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "results = loaded_model.predict(X_new)\n",
    "print('Predictions:')\n",
    "for prediction in results:\n",
    "    print(round(prediction,2))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
